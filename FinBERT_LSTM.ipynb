{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"executionInfo":{"elapsed":4526,"status":"ok","timestamp":1725226052688,"user":{"displayName":"James Wraw","userId":"02597508225589391651"},"user_tz":-60},"id":"taCz-BrS2AH7"},"outputs":[],"source":["%%capture\n","%pip install keras-tuner"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33673,"status":"ok","timestamp":1725226086356,"user":{"displayName":"James Wraw","userId":"02597508225589391651"},"user_tz":-60},"id":"PAsMzB6p0DXQ","outputId":"6c473d39-99bd-471f-cd3a-ac140459fdf4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","GPU is available and being used.\n"]}],"source":["from google.colab import drive\n","import pandas as pd\n","import keras\n","import scipy\n","import torch\n","import numpy as np\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout\n","from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n","from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n","from keras_tuner.tuners import RandomSearch\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","import datetime as datetime\n","import plotly.express as px\n","import matplotlib.pyplot as plt\n","\n","# Check if a GPU is available\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is available and being used.\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU is not available, using CPU instead.\")"]},{"cell_type":"code","source":["# Use google drive to save files - specify location here if local saving needed too.\n","drive.mount('/content/drive')\n","file_location = '/content/drive/MyDrive/Colab Notebooks/Dissertation/ic_fspml'\n","log_dir = f'{file_location}/models/' # TensorBoard logs"],"metadata":{"id":"DVpWPqDEyJ3x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jF2ytejI9s4A"},"source":["<h1>LSTM Modelling</h1>"]},{"cell_type":"markdown","metadata":{"id":"1j6_obDG0DXU"},"source":["<h2> Load in dataset </h2>"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":18765,"status":"ok","timestamp":1725226105117,"user":{"displayName":"James Wraw","userId":"02597508225589391651"},"user_tz":-60},"id":"U6OHXvSCxXGZ"},"outputs":[],"source":["# Load in prepared dataset, with news, sector, sentiment and price information populated\n","stock_news_and_price_df = pd.read_csv(f'{file_location}/data/ic_fspml_senti_price.csv')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1725226105118,"user":{"displayName":"James Wraw","userId":"02597508225589391651"},"user_tz":-60},"id":"5UnUQRWLakOy"},"outputs":[],"source":["# Function to get the top news tickers per sector (optional) based on the number of news articles associated\n","def get_top_news_tickers_per_sector(df, count_of_tickers, sector=''):\n","    if sector != '':\n","      df = df[df['sector']==sector]\n","\n","    # Group by sector and ticker, then count the number of news articles\n","    news_count_by_ticker = df.groupby(['sector', 'Ticker'])['article_headline'].count().reset_index(name='NewsCount')\n","\n","    # Sort the data within each sector by news count in descending order\n","    news_count_by_ticker = news_count_by_ticker.sort_values(by=['sector', 'NewsCount'], ascending=[True, False])\n","\n","    # Get the top x tickers per sector\n","    top_tickers_per_sector = news_count_by_ticker.groupby('sector').head(count_of_tickers)\n","\n","    return list(top_tickers_per_sector.Ticker)"]},{"cell_type":"markdown","metadata":{"id":"75Wx7STLPusW"},"source":["<h2> Preprocessing the Data </h2>"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1725226109500,"user":{"displayName":"James Wraw","userId":"02597508225589391651"},"user_tz":-60},"id":"H2lG4FG8GWjU"},"outputs":[],"source":["# Function to preprocess data for model training, by adding sentiment & scaling data\n","def preprocess_data(df, feature_columns, target_column):\n","    df = df.copy()\n","    # Map sentiment labels to numerical values, needed for input to LSTM\n","    sentiment_mapping = {'negative': -1, 'neutral': 0, 'positive': 1}\n","    df['sentiment'] = df['sentiment'].map(sentiment_mapping)\n","    df['sentiment'].fillna(0, inplace=True)\n","\n","    # Calculate the sum of sentiment per Date and sector\n","    sector_sentiment = df.groupby(['Date', 'sector'])['sentiment'].sum().reset_index()\n","    # Merge the sector sentiment back into the original DataFrame\n","    df = df.merge(sector_sentiment, on=['Date', 'sector'], suffixes=('', '_sector'))\n","\n","    # Calculate the sum of sentiment per Date, 'globally'\n","    global_sentiment = df.groupby(['Date'])['sentiment'].sum().reset_index()\n","    # Merge the global sentiment back into the original DataFrame\n","    df = df.merge(global_sentiment, on=['Date'], suffixes=('', '_global'))\n","\n","    # LSTMs are sensitive to the scale of the input data.\n","    # Normalising the features helps to stabilise and speed up the training process.\n","    if len(feature_columns) > 0:\n","      scaler = MinMaxScaler(feature_range=(0, 1))\n","      df[feature_columns] = scaler.fit_transform(df[feature_columns])\n","\n","    # Scale the target column seperately, so it can be used in the predictions to unscale\n","    price_scaler = MinMaxScaler(feature_range=(0, 1))\n","    df[target_column] = price_scaler.fit_transform(df[target_column])\n","\n","    df = df[feature_columns + target_column]\n","\n","    return df, price_scaler"]},{"cell_type":"markdown","metadata":{"id":"yo71g5_MRVYk"},"source":["<h3> Generate Sequences </h3>"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1725226114219,"user":{"displayName":"James Wraw","userId":"02597508225589391651"},"user_tz":-60},"id":"jPC7VmtDJZmE"},"outputs":[],"source":["# Function to filter input df to a specific stock\n","def filter_to_stock(df, stock_name):\n","    df = df[df['Ticker']==stock_name].copy()\n","    df.drop(columns='Ticker', inplace=True)\n","    return df\n","\n","\n","# Function to split the data into train, validate and test for features & target\n","# Returns the datasets as generators, for memory efficiency and ease of use with Keras\n","def create_generators(input_data, feature_columns, target_column, sequence_length=5, test_size=0.2, val_size=0.5, batch_size=16, shuffle=False):\n","\n","    # Extract features and target values from the input data\n","    features = input_data[feature_columns + target_column].values\n","    target = input_data[target_column].values\n","\n","    # Split the data into training, validation, and test sets before generating sequences\n","    X_train, X_temp, y_train, y_temp = train_test_split(features, target, test_size=test_size, shuffle=shuffle)\n","    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_size, shuffle=shuffle)\n","\n","    # Create the time series generators for each split. Note - DO NOT SHUFFLE\n","    train_generator = TimeseriesGenerator(X_train, y_train, length=sequence_length, batch_size=batch_size, shuffle=False)\n","    val_generator = TimeseriesGenerator(X_val, y_val, length=sequence_length, batch_size=batch_size, shuffle=False)\n","    test_generator = TimeseriesGenerator(X_test, y_test, length=sequence_length, batch_size=batch_size, shuffle=False)\n","\n","    return train_generator, val_generator, test_generator\n","\n","\n","# Function to generate datasets for multiple tickers and feature sets\n","# Returns datasets and scalers, named with ticker & feature set name.\n","def generate_datasets(df, tickers, feature_set_dict, target_column, sequence_length=5, test_size=0.2, val_size=0.5, shuffle=False):\n","    datasets = {}\n","    scalers = {}\n","\n","    # Loop over each feature set in the feature_set_dict\n","    for feature_set_name, feature_set_columns in feature_set_dict.items():\n","      # Loop over each ticker in the ticker list\n","      for ticker in tickers:\n","          # Filter and preprocess data per ticker\n","          filtered_df = filter_to_stock(df, ticker)\n","          processed_df, price_scaler = preprocess_data(filtered_df, feature_set_columns, target_column)\n","\n","          # Generate train, validate and test timeseries data\n","          train_generator, val_generator, test_generator = create_generators(\n","              processed_df, feature_set_columns, target_column, sequence_length, test_size, val_size, 32, shuffle\n","          )\n","\n","          # Store the timeseries and scalers in dictionaries\n","          datasets[ticker + feature_set_name] = (train_generator, val_generator, test_generator)\n","          scalers[ticker + feature_set_name] = price_scaler\n","\n","    return datasets, scalers"]},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":true,"executionInfo":{"elapsed":1390,"status":"ok","timestamp":1725226119452,"user":{"displayName":"James Wraw","userId":"02597508225589391651"},"user_tz":-60},"id":"J8WCFhCGK0NO"},"outputs":[],"source":["# Generate a list of top tickers with the most news articles per sector\n","# The function returns the top 2 tickers from each sector in the DataFrame\n","ticker_list = get_top_news_tickers_per_sector(stock_news_and_price_df, 2, sector='')\n","target_column = ['Adj Close']\n","\n","# Dictionary of feature sets to be used for generating datasets\n","# Each key is a feature set, with the associated list of feature columns\n","# The target column will automatically be included as input in each feature set\n","feature_set_dict = {'_close_only' : [],\n","                    '_open_close' : ['Open'],\n","                    '_open_close_sentiment' : ['Open', 'sentiment'],\n","                    '_open_close_sentiment_sector' : ['Open', 'sentiment', 'sentiment_sector'],\n","                    '_open_close_sentiment_sector_global' : ['Open', 'sentiment', 'sentiment_sector','sentiment_global']\n","                    }\n","\n","# Preprocess, then generate datasets and corresponding scalers for the specified tickers and feature sets\n","datasets, scalers = generate_datasets(stock_news_and_price_df, ticker_list, feature_set_dict, target_column)"]},{"cell_type":"markdown","metadata":{"id":"QO5K0ITMR4w4"},"source":["<h2> Create LSTM Model </h2>"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1725226120130,"user":{"displayName":"James Wraw","userId":"02597508225589391651"},"user_tz":-60},"id":"OeLcCJKaHSAb"},"outputs":[],"source":["# Load the TensorBoard notebook extension\n","# Can comment out without issue if not being used.\n","%load_ext tensorboard\n","%tensorboard --logdir '{file_location}/models/'"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1725226121703,"user":{"displayName":"James Wraw","userId":"02597508225589391651"},"user_tz":-60},"id":"-MbEwP6QA3vX"},"outputs":[],"source":["# Function to build the LSTM model with hyperparameter tuning using Keras-Tuner\n","def build_model(hp, input_shape):\n","    inputs = Input(shape=input_shape)\n","\n","    # Add the first LSTM layer. Units selected with hyperparameter tuning\n","    x = LSTM(units=hp.Int('units', min_value=32, max_value=512, step=32),\n","             return_sequences=True)(inputs)\n","\n","    # Add a Dropout layer to prevent overfitting. Dropout rate selected via hyperparameter tuning\n","    x = Dropout(rate=hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1))(x)\n","\n","    # Add the second LSTM layer. Units selected with hyperparameter tuning\n","    x = LSTM(units=hp.Int('units', min_value=32, max_value=512, step=32))(x)\n","\n","    # Add the second Dropout layer to prevent overfitting. Dropout rate selected via hyperparameter tuning\n","    x = Dropout(rate=hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1))(x)\n","\n","    # Dense output layer with a single unit, for adj close price prediction\n","    outputs = Dense(1)(x)\n","\n","    # Create and compile the model, with Adam optimiser, and MSE for loss function to be optimised\n","    model = Model(inputs=inputs, outputs=outputs)\n","    model.compile(optimizer='adam', loss='mean_squared_error')\n","\n","    return model\n","\n","\n","# Function to train and evaluate the model using hyperparameter tuning\n","def train_and_evaluate_model(dataset_name, train_generator, val_generator, test_generator):\n","    # Automatically calculate input_shape from the generator, from the shape of the features (timesteps, num_features)\n","    input_shape = train_generator[0][0].shape[1:]\n","\n","    # Define the Keras-Tuner per dataset name, passing input_shape to build_model, finding the best hyperparameters\n","    tuner = RandomSearch(\n","        lambda hp: build_model(hp, input_shape),\n","        objective='val_loss',\n","        max_trials=10,\n","        executions_per_trial=3,\n","        directory=f'{file_location}/models',\n","        project_name=f'stock_price_prediction_{dataset_name}'\n","    )\n","\n","    # Start the hyperparameter search - epochs set to 150, as early stopping will prevent excessive fruitless runs\n","    tuner.search(train_generator, epochs=150, validation_data=val_generator,\n","                 callbacks=[\n","                     # Log training metrics for visualisation in TensorBoard, like ensuring raining loss convergence\n","                     TensorBoard(log_dir=f'{file_location}/models/logs_{dataset_name}', histogram_freq=1),\n","                     # Early stopping to prevent overfitting, with restoration of the best model weights. High patience, as GPUs are available.\n","                     EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n","                 ]\n","    )\n","\n","    # Retrieve the best model found by Keras-Tuner\n","    best_model = tuner.get_best_models(num_models=1)[0]\n","\n","    # Output & evaluate the model on the test set using the test generator\n","    test_loss = best_model.evaluate(test_generator)\n","    print(f'{dataset_name} Test Loss: {test_loss}')\n","\n","    # Save the best model with the dataset name in the filename\n","    best_model.save(f'{file_location}/models/ic_fspml_{dataset_name}_model.keras')\n","\n","    return best_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wS0AavUyCcw6"},"outputs":[],"source":["best_models = {}\n","\n","# Loop over each of the datasets, training and validating the model, and storing the best model in the dictionary.\n","for dataset_name, (train_generator, val_generator, test_generator) in datasets.items():\n","    print(f\"Training and evaluating model for {dataset_name}...\")\n","    best_model = train_and_evaluate_model(dataset_name, train_generator, val_generator, test_generator)\n","    best_models[dataset_name] = best_model"]},{"cell_type":"markdown","metadata":{"id":"8B9m4Dh-SxyH"},"source":["<h1> Model Evaluation </h1>"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":1188,"status":"ok","timestamp":1725242064877,"user":{"displayName":"James Wraw","userId":"02597508225589391651"},"user_tz":-60},"id":"i8vvo1V2HXCX"},"outputs":[],"source":["# Function to get model predictions and true values from the test generator\n","def get_model_predictions(model, test_generator, scaler):\n","    # Get predicted output from the model and test generator\n","    y_pred = model.predict(test_generator)\n","\n","    # Extract all true values (y) from the generator\n","    y_true = []\n","    for i in range(len(test_generator)):\n","        x, y = test_generator[i]\n","        y_true.extend(y)\n","\n","    # Inverse transform the predictions and true values to their original scale\n","    y_true = np.array(y_true)\n","    y_true = scaler.inverse_transform(y_true.reshape(-1, 1)).flatten()\n","    y_pred = scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n","\n","    return y_true, y_pred\n","\n","\n","# Function to calculate the mean percentage error\n","def percentage_error(y_true, y_pred):\n","    y_true, y_pred = np.array(y_true), np.array(y_pred)\n","    # Calculate the absolute error\n","    error = np.abs(y_true - y_pred)\n","    # Calculate the percentage error\n","    percentage_error = (error / np.abs(y_true)) * 100\n","\n","    return np.mean(percentage_error)\n","\n","\n","# Function to evaluate model performance using various metrics\n","def evaluate_model_performance(y_true, y_pred):\n","    mae = mean_absolute_error(y_true, y_pred)\n","    mse = mean_squared_error(y_true, y_pred)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_true, y_pred)\n","    percent_errors = percentage_error(y_true, y_pred)\n","\n","    # Return the metrics as a dictionary\n","    metrics = {\n","        'mse': mse,\n","        'rmse': rmse,\n","        'mae': mae,\n","        'r2': r2,\n","        'percentage_error': percent_errors\n","    }\n","\n","    return metrics\n","\n","\n","# Function to create a comparison table of model performance metrics\n","def create_comparison_table(results):\n","    # Initialise an empty dictionary to store metrics for each model\n","    comparison_data = {}\n","\n","    # Populate the dictionary with metrics from evaluate_model_performance, for each model\n","    for model_name, data in results.items():\n","        comparison_data[model_name] = data['metrics']\n","\n","    # Transpose the DataFrame so that models are columns and metrics are rows\n","    comparison_df = pd.DataFrame(comparison_data)\n","    comparison_df = comparison_df.transpose()\n","    comparison_df = comparison_df.reset_index()\n","    comparison_df = comparison_df.rename(columns={'index': 'ModelName'})\n","\n","    # Save performance metrics for future use\n","    comparison_df.to_csv(f'{file_location}/results/performance_metrics.csv', index=False)\n","\n","    return comparison_df.sort_values('ModelName')\n","\n","\n","# Function to plot and compare actual vs predicted stock prices\n","def plot_predictions(y_true, y_predicted):\n","    plt.figure(figsize=(12, 6))\n","    plt.plot(y_true, label='Actual')\n","    plt.plot(y_predicted, label='Predicted')\n","    plt.title('Actual vs Predicted Stock Prices')\n","    plt.xlabel('Time')\n","    plt.ylabel('Stock Adjusted Close Price')\n","    plt.legend()\n","    plt.savefig(f'{file_location}/results/plot_{dataset_name}.png')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eRT5Rx_kEqpz"},"outputs":[],"source":["results = {}\n","\n","# Loop over each dataset in datasets, generate performance metrics\n","for dataset_name, (train_generator, val_generator, test_generator) in datasets.items():\n","    print(f\"Evaluating performance for {dataset_name}...\")\n","    best_model = best_models[dataset_name]\n","    y_true, y_predicted = get_model_predictions(best_model, test_generator, scalers[dataset_name])\n","    metrics = evaluate_model_performance(y_true, y_predicted)\n","\n","    results[dataset_name] = {\n","        'model': best_model,\n","        'metrics': metrics\n","    }\n","\n","# Create a comparison table of the performance metrics across all datasets\n","comparison_table = create_comparison_table(results)\n","comparison_table"]},{"cell_type":"markdown","source":["<h1> Plotting Results <h1>"],"metadata":{"id":"pB1wEqMlYR69"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qIqbKl-aLata","collapsed":true},"outputs":[],"source":["for dataset_name, (train_generator, val_generator, test_generator) in datasets.items():\n","    print(f\"Evaluating performance for {dataset_name}...\")\n","    best_model = best_models[dataset_name]\n","    y_true, y_predicted = get_model_predictions(best_model, test_generator, scalers[dataset_name])\n","    plot_predictions(y_true, y_predicted)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}
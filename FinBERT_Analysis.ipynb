{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"taCz-BrS2AH7"},"outputs":[],"source":["%%capture\n","%pip install datasets\n","%pip install yfinance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28398,"status":"ok","timestamp":1725230557162,"user":{"displayName":"James Wraw","userId":"02597508225589391651"},"user_tz":-60},"id":"PAsMzB6p0DXQ","outputId":"8c33789e-c661-4622-e865-6f1457a06ad8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","GPU is available and being used.\n"]}],"source":["import pandas as pd\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from datasets import load_dataset\n","import numpy as np\n","import yfinance as yf\n","from tqdm import tqdm\n","from google.colab import drive\n","import os\n","import datetime\n","import torch\n","\n","# Use tqdm to time pandas functions\n","tqdm.pandas()\n","\n","# Use google drive to save files\n","drive.mount('/content/drive')\n","\n","# Check if a GPU is available\n","if torch.cuda.is_available():\n","  device = torch.device(\"cuda\")\n","  print(\"GPU is available and being used.\")\n","else:\n","  device = torch.device(\"cpu\")\n","  print(\"GPU is not available, using CPU instead.\")"]},{"cell_type":"code","source":["file_location = '/content/drive/MyDrive/Colab Notebooks/Dissertation/ic_fspml/data'"],"metadata":{"id":"QTICu1X4jFc6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1j6_obDG0DXU"},"source":["<h1> Load in datasets </h1>"]},{"cell_type":"code","source":["# Loading in the ic_fspml dataset, as a dataframe\n","def load_ic_fspml_dataset():\n","    # Define the file paths for the train, validation, and test splits to be loaded in a dictionary\n","    splits = {'train': 'data/train-00000-of-00001-161ce92db66dabb3.parquet',\n","              'validation': 'data/validation-00000-of-00001-7588e28a0ed7e31b.parquet',\n","              'test': 'data/test-00000-of-00001-d9fc71e4a9b02e72.parquet'}\n","\n","    # Read in the parquet files, and combine them into one dataframe.\n","    df1 = pd.read_parquet(\"hf://datasets/ic-fspml/stock_news_sentiment/\" + splits[\"train\"])\n","    df2 = pd.read_parquet(\"hf://datasets/ic-fspml/stock_news_sentiment/\" + splits[\"validation\"])\n","    df3 = pd.read_parquet(\"hf://datasets/ic-fspml/stock_news_sentiment/\" + splits[\"test\"])\n","    combined_df = pd.concat([df1, df2, df3])\n","\n","    return combined_df\n","\n","# Function to get high level information about loaded dataset\n","def get_dataset_stats(df, dataset_name, date_col):\n","    tickers = get_tickers(df)\n","    print('Dataset ' + dataset_name + 'contains ' + str(len(tickers)) + ' unique tickers.')\n","    print('Dataset spans from ' + str(df[date_col].min()) + ' to ' + str(df[date_col].max()))\n","    print('Dataset shape: ' + str(df.shape))"],"metadata":{"id":"Kxrm8j6nFBs4","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to get the list of unique tickers in the dataset\n","def get_tickers(df):\n","    tickers = df['ticker'].unique().tolist()\n","    return tickers\n","\n","\n","# Function to get sector and type information for a list of tickers using yfinance\n","def get_stock_info(tickers):\n","    # Get the sector and type from yFinance info. Returns list of dictionaries.\n","    stock_info = []\n","    for ticker in tickers:\n","        stock = yf.Ticker(ticker)\n","        info = {'ticker': ticker, 'sector': stock.info.get('sector'), 'type': stock.info.get('quoteType')}\n","        stock_info.append(info)\n","\n","    # Return a DataFrame from the list of dictionaries\n","    return pd.DataFrame(stock_info)\n","\n","\n","# Function to get stock prices for a list of tickers within a specific date range\n","def get_stock_prices(tickers, start_date, end_date):\n","    # YFinance requires start_date and end_date to be datetime objects in date format\n","    start_date = pd.to_datetime(start_date).date()\n","    end_date = pd.to_datetime(end_date).date()\n","    end_date = pd.to_datetime(end_date) + datetime.timedelta(days=1)\n","\n","    # Query the available daily stock prices from yfinance - note, this is slow, and produces warning output for failed downloads\n","    df = yf.download(tickers, start=start_date, end=end_date)\n","\n","    # Select specific columns ('Open', 'Close', 'Adj Close', 'Volume') and stack them\n","    df = pd.DataFrame(df[['Open','Close','Adj Close', 'Volume']].stack()).reset_index()\n","    df['DayChange'] = df['Open'] - df['Adj Close']\n","\n","    return df\n","\n","\n","# Function to merge stock prices with additional stock information (sector and type)\n","def merge_stock_prices_with_info(stock_info, stock_prices):\n","    # Drop rows in stock_info where either 'sector' or 'type' is missing - likely stocks have been delisted.\n","    stock_info = stock_info.dropna()\n","\n","    # Left join to ensure again that all stock prices are retained\n","    df = stock_prices.merge(stock_info, left_on='Ticker', right_on='ticker', how='left')\n","\n","    return df\n","\n","\n","# Function to save the stock prices DF to a CSV file\n","def save_stock_prices(df, file_source):\n","    df.to_csv(f'{file_location}/{file_source}_prices.csv', index=False)"],"metadata":{"id":"7tl_IJoyo9Nh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h1> Sentiment Analysis </h1>"],"metadata":{"id":"N40CtRfzFMLn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"isXQs_n8eyLY","collapsed":true},"outputs":[],"source":["# Function to load in the finbert sentiment analysis tokenizer and model\n","def load_in_bert_model():\n","    # https://huggingface.co/transformers/v3.0.2/model_doc/bert.html\n","    tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")\n","    bert_model = BertForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n","\n","    return tokenizer, bert_model\n","\n","tokenizer, bert_model = load_in_bert_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qpOghrHwAhUE"},"outputs":[],"source":["# Function to use finbert to extract the sentiment in a string\n","def sentiment_analysis_bert(text):\n","    # Move the FinBERT model to the device loaded earlier - ideally this will be a GPU if available\n","    bert_model.to(device)\n","\n","    # Tokenize the input text, and move to device (CPU or GPU)\n","    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","\n","    # Perform inference - no need for gradients as we're not training the FinBERT model. Saves memory.\n","    with torch.no_grad():\n","      outputs = bert_model(**inputs)\n","\n","    # Extract the predicted probabilities\n","    probs = outputs.logits.softmax(dim=1).detach().cpu().numpy()[0]  # Move the output back to CPU for further processing\n","\n","    # Define and assign the predicted sentiment label, based on highes probability\n","    sentiment_labels = ['negative', 'neutral', 'positive']\n","    predicted_sentiment = sentiment_labels[np.argmax(probs)]\n","\n","    return predicted_sentiment\n","\n","\n","# Function to apply sentiment analysis to a DataFrame in batches and save the results\n","# Useful to ensure output is saved, as limited computer resources were available\n","def apply_sentiment_analysis_to_df(df, col_to_analyse, file_source, batch_size=50000):\n","    # Split the DataFrame into batches of 50,000 rows\n","    num_batches = (len(df) + batch_size - 1) // batch_size\n","\n","    # Iterate through each batch and extract the sentiment\n","    for i in range(num_batches):\n","      start_idx = i * batch_size\n","      end_idx = min((i + 1) * batch_size, len(df))\n","      batch = df.iloc[start_idx:end_idx]\n","\n","      # Apply sentiment analysis to the batch\n","      batch['sentiment'] = batch[col_to_analyse].progress_apply(sentiment_analysis_bert)\n","\n","      # Save the batch to a CSV file\n","      batch.to_csv(f'{file_location}/{file_source}_{i}_senti.csv', index=False)\n","\n","      print('Completed batch ' + str(i) + ' which spans from ' + str(start_idx) + ' to ' + str(end_idx))\n","\n","\n","# Function to combine all CSV files in a directory into a single DataFrame and save the result\n","def combine_csv_files(file_source):\n","    folder_path = f'{file_location}/'\n","    combined_df = pd.DataFrame()\n","    for filename in os.listdir(folder_path):\n","      if filename.endswith('_senti.csv'):\n","        file_path = os.path.join(folder_path, filename)\n","        df = pd.read_csv(file_path)\n","        combined_df = pd.concat([combined_df, df])\n","\n","    # Save the combined file to a new CSV\n","    combined_df.to_csv(f'{file_location}/{file_source}_sentiment.csv', index=False)\n","\n","    return combined_df"]},{"cell_type":"markdown","source":["<h1> Load & merge the collected and analysed data </h1>"],"metadata":{"id":"oVUsBumVMSYx"}},{"cell_type":"code","source":["# Function to load the saved csv with tickers, prices and stock info.\n","def load_prices_df(file_source):\n","    df = pd.read_csv(f'{file_location}/{file_source}_prices.csv')\n","    return df\n","\n","\n","# Function to load the sentiment DataFrame from a CSV file\n","def load_sentiment_df(file_source):\n","    df = pd.read_csv(f'{file_location}/{file_source}_sentiment.csv')\n","    return df\n","\n","\n","# Function to merge the sentiment and stock prices DataFrames\n","def merge_sentiment_and_prices(sentiment_df, prices_df):\n","    sentiment_df['Date'] = pd.to_datetime(sentiment_df['article_date']).dt.date\n","    prices_df['Date'] = pd.to_datetime(prices_df['Date']).dt.date\n","    full_df = prices_df.drop(columns=['ticker']).merge(sentiment_df.drop(columns=['type','sector','label']), left_on=['Date','Ticker'], right_on=['Date','ticker'], how='outer')\n","    return full_df.drop(columns=['ticker'])\n","\n","\n","# Function to save the combined sentiment and price DataFrame to a CSV file\n","def save_senti_prices(df, file_source):\n","    df.to_csv(f'{file_location}/{file_source}_senti_price.csv', index=False)"],"metadata":{"id":"nueeibCFO78A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<h1> Run block - Run all to generate dataset from scratch</h1>"],"metadata":{"id":"ODpz23_5ntgV"}},{"cell_type":"code","source":["ic_fspml_df = load_ic_fspml_dataset() # Load in dataset\n","tickers = get_tickers(ic_fspml_df) # Get list of unique tickers\n","stock_info = get_stock_info(tickers) # Get stock info for all tickers - note, expect a lot of warnings"],"metadata":{"id":"EJr3Up4MqC6x","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cut the tickers down to correctly populated equities only\n","tickers = stock_info[(stock_info['type']=='EQUITY') & (stock_info['sector'].notna())]['ticker'].unique().tolist()\n","\n","# Get stock prices for tickers\n","ic_fspml_df = ic_fspml_df[ic_fspml_df['ticker'].isin(tickers)]\n","stock_prices = get_stock_prices(tickers, ic_fspml_df['article_date'].min(), ic_fspml_df['article_date'].max())"],"metadata":{"id":"LoIZOIaxxZ3t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge with the info df, and save the result. Saving takes a few mins.\n","stock_price_and_info = merge_stock_prices_with_info(stock_info, stock_prices)\n","save_stock_prices(stock_price_and_info, 'ic_fspml')\n","\n","apply_sentiment_analysis_to_df(ic_fspml_df, 'article_headline', 'ic_fspml') # Apply sentiment analysis to the headlines, save as CSV\n","sentiment_df = combine_csv_files('ic_fspml') # Load & combine the CSV files into a single DataFrame"],"metadata":{"id":"KRGCqoipqC1e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge and save the price, stock info and sentiment.\n","full_price_and_sentiment_df = merge_sentiment_and_prices(sentiment_df, stock_price_and_info)\n","full_price_and_sentiment_df = full_price_and_sentiment_df.sort_values(['Date', 'Ticker'])\n","save_senti_prices(full_price_and_sentiment_df, 'ic_fspml')"],"metadata":{"id":"tMS51LSJrW3j","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# full_price_and_sentiment_df[~full_price_and_sentiment_df['sentiment'].isna()].head()"],"metadata":{"collapsed":true,"id":"KUfSbh13-mi3","executionInfo":{"status":"ok","timestamp":1725258050572,"user_tz":-60,"elapsed":617,"user":{"displayName":"James Wraw","userId":"02597508225589391651"}}},"execution_count":1,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1hmeYZtab9zV9gX_GqZDcMFpJ15DG36fb","timestamp":1722784321219}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}